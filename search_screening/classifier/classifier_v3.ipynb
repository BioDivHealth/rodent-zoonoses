{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f20ccd6-ef00-4002-bfd6-7ecd64ed6f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import litstudy as lst\n",
    "import nltk\n",
    "#import Lbl2Vec as vec\n",
    "import pandas as pd\n",
    "import numpy as np#for text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score# bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer#for word embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "785ccc88-4c12-4abe-b901-de568cfdc9ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title               0\n",
       "Authors            22\n",
       "Abstract          107\n",
       "Published Year      1\n",
       "Accepted            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master = pd.read_csv(\"captive_wild_animals.csv\")\n",
    "master = master[[\"Title\", \"Authors\", \"Abstract\", \"Published Year\", \"Accepted\"]]\n",
    "master.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3ee218d-d03c-422a-a6ea-cb8449ddaa9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title             0\n",
       "Authors           0\n",
       "Abstract          0\n",
       "Published Year    0\n",
       "Accepted          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master = master.dropna(ignore_index=True)\n",
    "master.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e39287cd-2c15-4b72-b695-bede29014b00",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### remove stopwords and punctuation.\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize as tokenize\n",
    "\n",
    "words = tokenize(master[\"abstract\"][1])\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "extra = {\",\", \".\", \"(\", \")\", \";\", \":\", \"=\", \"[0-9]\"}\n",
    "filtered_list = []\n",
    "for word in words:\n",
    "    if word.casefold() not in stop_words and word.casefold() not in extra:\n",
    "         filtered_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63702497-1c3a-4b1a-b305-546695cb5b82",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### stemming\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stemmed = [stemmer.stem(word) for word in filtered_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c9ebca1-0e6f-4dec-add2-dc080bcfef1b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "abstracts = master[\"abstract\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9131ad7-7248-4dc5-a3b8-1c5f9fab8d5e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize as tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "nlp_dict = {}\n",
    "all_words = []\n",
    "row = int(0)\n",
    "for abstract in abstracts:\n",
    "    words = tokenize(abstract)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    extra = {\",\", \".\", \"(\", \")\", \";\", \":\", \"=\"}\n",
    "    filtered_list = []\n",
    "    for word in words:\n",
    "        if word.casefold() not in stop_words and word.casefold() not in extra:\n",
    "             filtered_list.append(word)\n",
    "    \n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed = [stemmer.stem(word) for word in filtered_list]\n",
    "    nlp_dict[row] = stemmed\n",
    "    all_words.append(stemmed)\n",
    "    row += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d99b74fa-f14a-48ed-a7d0-923584c5c0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\harrg\\AppData\\Local\\Temp\\ipykernel_276\\925071319.py:7: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  text = re.sub('\\s+', ' ', text)\n"
     ]
    }
   ],
   "source": [
    "#convert to lowercase, strip and remove punctuations\n",
    "def preprocess(text):\n",
    "    text = text.lower() \n",
    "    text=text.strip()  \n",
    "    text=re.compile('<.*?>').sub('', text) \n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  \n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text) \n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) \n",
    "    text = re.sub(r'\\s+',' ',text) \n",
    "    return text\n",
    " \n",
    "# STOPWORD REMOVAL\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)#LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    " \n",
    "# This is a helper function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN# Tokenize the sentence\n",
    "\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6585736-5893-4ab7-9574-d610019ff1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(preprocess(string)))\n",
    "\n",
    "master['abstract_clean'] = master['Abstract'].apply(lambda x: finalpreprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d973f3a4-55b6-4afc-aeb2-7938c8ba5672",
   "metadata": {},
   "outputs": [],
   "source": [
    "### generate features\n",
    "# trimmed abstract word count\n",
    "list = []\n",
    "for row in range(len(master)):\n",
    "    list.append(len(master[\"abstract_clean\"][row].split()))\n",
    "master.insert(5, \"abs_len\", list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1592a2da-c24f-425f-96ea-4c74809e1b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num stopwords\n",
    "list = []\n",
    "for row in range(len(master)):\n",
    "    list.append(len(master[\"Abstract\"][row].split()) - len(master[\"abstract_clean\"][row].split()))\n",
    "master.insert(6, \"n_stopwords\", list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f7625e8-43ba-4451-aa17-87d8f3bd5e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# title length\n",
    "list = []\n",
    "for row in range(len(master)):\n",
    "    list.append(len(master[\"Title\"][row].split()))\n",
    "master.insert(7, \"title_len\", list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a186295-2af6-4cb6-8578-e5c51dc11d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trimmed abastract word length average\n",
    "list = []\n",
    "for row in range(len(master)):\n",
    "    list.append(sum(len(word) for word in master[\"abstract_clean\"][row].split()) / len(master[\"abstract_clean\"][row].split()))\n",
    "master.insert(8, \"abs_word_len\", list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bd4fd77-2a3e-43c0-9c8a-867f7cabd626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# age/year published\n",
    "master['age'] = 2024 - master['Published Year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb4b531e-6f77-4e59-8887-c526572fe3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of authors\n",
    "list = []\n",
    "for row in range(len(master)):\n",
    "    list.append(len(master[\"Authors\"][row].split(\";\")))\n",
    "master.insert(11, \"n_authors\", list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "716b9f8d-257a-4203-a319-b6632d18ee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword based features (abstract_clean)\n",
    "keywords = ['species', 'conservation', 'wildlife', 'farm', 'fur', 'game', 'ecotourism', 'wild', 'animal', 'farming', 'animals', 'zoo', 'zoonosis', 'zoonotic', 'virus', 'vector']\n",
    "for word in keywords:\n",
    "    list=[]\n",
    "    for row in range(len(master)):\n",
    "        list.append(int(word in master[\"abstract_clean\"][row]))\n",
    "    master.insert(12, f\"abs_{word}\", list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47ef3271-e4ae-4e82-a83d-4396c2be1f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword based features (title)\n",
    "for word in keywords:\n",
    "    list=[]\n",
    "    for row in range(len(master)):\n",
    "        list.append(int(word in master[\"Title\"][row]))\n",
    "    master.insert(12, f\"ttl_{word}\", list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7841b77-b3e7-4ca3-af83-9d8168ee80e5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "final = master[['Accepted', 'abs_len',\n",
    "       'n_stopwords', 'title_len', 'abs_word_len', 'age',\n",
    "       'n_authors', 'ttl_vector', 'ttl_virus', 'ttl_zoonotic', 'ttl_zoonosis',\n",
    "       'ttl_zoo', 'ttl_animals', 'ttl_farming', 'ttl_animal', 'ttl_wild',\n",
    "       'ttl_ecotourism', 'ttl_game', 'ttl_fur', 'ttl_farm', 'ttl_wildlife',\n",
    "       'ttl_conservation', 'ttl_species', 'abs_vector', 'abs_virus',\n",
    "       'abs_zoonotic', 'abs_zoonosis', 'abs_zoo', 'abs_animals', 'abs_farming',\n",
    "       'abs_animal', 'abs_wild', 'abs_ecotourism', 'abs_game', 'abs_fur',\n",
    "       'abs_farm', 'abs_wildlife', 'abs_conservation', 'abs_species']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d7fac2e0-90b1-4731-bbe2-b8d047ce347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_other = master[['abs_len',\n",
    "       'n_stopwords', 'title_len', 'abs_word_len', 'age',\n",
    "       'n_authors'\n",
    "        #, 'ttl_vector', 'ttl_virus', 'ttl_zoonotic', 'ttl_zoonosis',\n",
    "      # 'ttl_zoo', 'ttl_animals', 'ttl_farming', 'ttl_animal', 'ttl_wild',\n",
    "    #   'ttl_ecotourism', 'ttl_game', 'ttl_fur', 'ttl_farm', 'ttl_wildlife',\n",
    "     #  'ttl_conservation', 'ttl_species', 'abs_vector', 'abs_virus',\n",
    "     #  'abs_zoonotic', 'abs_zoonosis', 'abs_zoo', 'abs_animals', 'abs_farming',\n",
    "     #  'abs_animal', 'abs_wild', 'abs_ecotourism', 'abs_game', 'abs_fur',\n",
    "     #  'abs_farm', 'abs_wildlife', 'abs_conservation', 'abs_species'\n",
    "       \n",
    "        ]]\n",
    "y = master['Accepted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ab068c4f-f6ea-4109-9138-92f62b51baba",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_tok= [nltk.word_tokenize(i) for i in master[\"abstract_clean\"]]  \n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(master['abstract_clean'])\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "master['abstract_clean_tok']=[nltk.word_tokenize(i) for i in master['abstract_clean']]\n",
    "model = Word2Vec(master['abstract_clean_tok'],min_count=1)     \n",
    "w2v = dict(zip(model.wv.index_to_key, model.wv.vectors))\n",
    "modelw = MeanEmbeddingVectorizer(w2v)# converting text to numerical data using Word2Vec\n",
    "X_vectors_w2v = modelw.transform(abs_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a5091f8f-4c21-43f7-a082-0bf44efe2fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5534, 29925)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_train_vectors_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "970a7332-848d-4cbc-95d7-f1c8bfed4e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5534, 6)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d3ac5e24-1c10-4d59-942d-203c7be83f01",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 0 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[107], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_other\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_vectors_tfidf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m np\u001b[38;5;241m.\u001b[39mshape(X)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\litsearch\\Lib\\site-packages\\numpy\\lib\\function_base.py:5618\u001b[0m, in \u001b[0;36mappend\u001b[1;34m(arr, values, axis)\u001b[0m\n\u001b[0;32m   5616\u001b[0m     values \u001b[38;5;241m=\u001b[39m ravel(values)\n\u001b[0;32m   5617\u001b[0m     axis \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 5618\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 0 dimension(s)"
     ]
    }
   ],
   "source": [
    "X = np.append(X_other, X_train_vectors_tfidf, axis=1)\n",
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7b79cfa4-4e70-44fb-964e-741fe115f507",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLITTING THE TRAINING DATASET INTO TRAIN AND TEST\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1, shuffle=True)#Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e6c59d61-0aa9-416f-93c4-451938c4334d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.27      0.43       524\n",
      "           1       0.07      0.97      0.13        30\n",
      "\n",
      "    accuracy                           0.31       554\n",
      "   macro avg       0.53      0.62      0.28       554\n",
      "weighted avg       0.94      0.31      0.41       554\n",
      "\n",
      "Confusion Matrix: [[143 381]\n",
      " [  1  29]]\n",
      "AUC: 0.8075699745547074\n"
     ]
    }
   ],
   "source": [
    "# Initializing the Logistic Regression model\n",
    "model = LogisticRegression(solver = 'liblinear', C=0.1, penalty = 'l2', class_weight={0: 1., 1: 100.})\n",
    "\n",
    "# Training the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_pred))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)\n",
    "\n",
    "# Calculating accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d244c8-5ca0-47c2-bd05-1680fd74c191",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "litsearch",
   "language": "python",
   "name": "litsearch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
